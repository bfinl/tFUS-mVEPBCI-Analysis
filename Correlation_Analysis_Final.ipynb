{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4de783",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "Written by primarily by Joshua Kosnoff. MNE registration functions are heavily copied from MNE tutorials. Any additional credit is given in function headers \n",
    "\n",
    "To get a stable connectivity measurement for lower frequencies (i.e. theta; our main area of interest), we need to increase the epoch length. This means we have to start over with all of our preprocessing. We will do the exact same things, just with larger windows. For safety, I used -500 ms to 1500 ms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44607e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# CHANGE THESE PATHS - the relative paths for box should be the same. You may need to change\n",
    "# the relative FreeSurfer paths if you have different nomenclature or organization methods\n",
    "\n",
    "path_to_file_paths = r\"file_paths_remapped.json\"\n",
    "path_to_box = r\"/Users/jkosnoff/Library/CloudStorage/Box-Box/Joshua-Speller-Paper/Data/EEG_Data/\"\n",
    "\n",
    "path_to_freesurfer = path_to_box + r'/Freesurfer'\n",
    "path_to_freesurfer = r\"/Users/jkosnoff/Docs/Freesurfer\"\n",
    "\n",
    "parse_mode = 'per_scan'\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4db24c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c9626",
   "metadata": {},
   "source": [
    "## Source Extraction\n",
    "\n",
    "\n",
    "Load in the preprocessed DF so we don't need to run that script again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e158dbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>condition</th>\n",
       "      <th>data_epoch</th>\n",
       "      <th>hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subj01</td>\n",
       "      <td>Non-Modulated</td>\n",
       "      <td>&lt;EpochsArray |  1 events (all good), -0.2 – 0....</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subj01</td>\n",
       "      <td>Non-Modulated</td>\n",
       "      <td>&lt;EpochsArray |  1 events (all good), -0.2 – 0....</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subj01</td>\n",
       "      <td>Non-Modulated</td>\n",
       "      <td>&lt;EpochsArray |  1 events (all good), -0.2 – 0....</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subj01</td>\n",
       "      <td>Non-Modulated</td>\n",
       "      <td>&lt;EpochsArray |  1 events (all good), -0.2 – 0....</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subj01</td>\n",
       "      <td>Non-Modulated</td>\n",
       "      <td>&lt;EpochsArray |  1 events (all good), -0.2 – 0....</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject      condition                                         data_epoch  \\\n",
       "0  Subj01  Non-Modulated  <EpochsArray |  1 events (all good), -0.2 – 0....   \n",
       "1  Subj01  Non-Modulated  <EpochsArray |  1 events (all good), -0.2 – 0....   \n",
       "2  Subj01  Non-Modulated  <EpochsArray |  1 events (all good), -0.2 – 0....   \n",
       "3  Subj01  Non-Modulated  <EpochsArray |  1 events (all good), -0.2 – 0....   \n",
       "4  Subj01  Non-Modulated  <EpochsArray |  1 events (all good), -0.2 – 0....   \n",
       "\n",
       "    hit  \n",
       "0  True  \n",
       "1  True  \n",
       "2  True  \n",
       "3  True  \n",
       "4  True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocessed_df = pd.read_pickle(f\"Analysis_Rerun.pkl\")\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0039aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>1: 1</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-0.200 – 0.500 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsArray |  1 events (all good), -0.2 – 0.5 s, baseline off, ~126 kB, data loaded,\n",
       " '1': 1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.data_epoch.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbcb21d",
   "metadata": {},
   "source": [
    "## EEG Source Imaging Extraction\n",
    "\n",
    "The average series for each subject's V5, V1, superior parietal lobe, and inferior temporal gyrus is extracted using EEG source imaging techniques and their corresponding FreeSurfer labeled regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1abb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESI functions, cribbed/repurposed from MNE Python tutorials\n",
    "import os.path \n",
    "import json\n",
    "import copy\n",
    "\n",
    "# Third Party Packages\n",
    "import statsmodels.stats.multitest\n",
    "import itertools\n",
    "from math import comb\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import shutil\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import (make_axes_locatable, ImageGrid,\n",
    "                                     inset_locator)\n",
    "\n",
    "\n",
    "def make_digitization_fif(subject, subject_dir, path_to_vdhr, path_to_RAS, session_num = \"\"):\n",
    "    raw = mne.io.read_raw_brainvision(path_to_vdhr,misc=['ECG', 'EMG', 'FootPad'],preload=False)\n",
    "    mont = mne.channels.read_dig_localite(path_to_RAS)\n",
    "    raw.set_montage(mont, match_case=False, on_missing='warn')\n",
    "    raw.save(fr\"{subject_dir}/{subject}_{session_num}_eeg.fif\", overwrite=True)\n",
    "    return\n",
    "\n",
    "def auto_coregister(subject, subject_dir, subject_raw, plot_coreg = True, save_file = True, session_num=\"\"):\n",
    "    # Note: Before running this, you need to make the BEM model for the subject. See the FreeSurfer Protocol \n",
    "    # for the terminal commands on how to do this\n",
    "    \n",
    "    \n",
    "    # Cribbed from https://mne.tools/dev/auto_tutorials/forward/25_automated_coreg.html.\n",
    "    # Reformatted into a function by Joshua Kosnoff.\n",
    "    info = mne.io.read_info(fr\"{subject_dir}/{subject_raw}\")\n",
    "    plot_kwargs = dict(\n",
    "        subject=subject,\n",
    "        subjects_dir=subject_dir,\n",
    "        surfaces=\"head\",\n",
    "        dig=True,\n",
    "        eeg=\"projected\",\n",
    "        meg=[],\n",
    "        show_axes=True,\n",
    "        coord_frame=\"auto\",\n",
    "    )\n",
    "    view_kwargs = dict(azimuth=30, elevation=90, distance=0.6, focalpoint=(0.0, 0.0, 0.0))\n",
    "\n",
    "    fiducials = \"auto\"  # get fiducials from fsaverage\n",
    "    \n",
    "    coreg = mne.coreg.Coregistration(info, subject, subject_dir, fiducials=fiducials)\n",
    "    coreg.omit_head_shape_points(distance=5.0 / 1000)  # distance is in meters\n",
    "    coreg.fit_icp(n_iterations=30, nasion_weight=10.0, verbose=True)\n",
    "    \n",
    "    # Visualize the coregistration\n",
    "    if plot_coreg:\n",
    "        fig = mne.viz.plot_alignment(info, trans=coreg.trans, **plot_kwargs)\n",
    "        mne.viz.set_3d_view(fig, **view_kwargs)\n",
    "\n",
    "    # Save the file to the subject directory\n",
    "    if save_file:\n",
    "        mne.write_trans(fr'{subject_dir}/{subject}_{session_num}-trans.fif', coreg.trans, overwrite=True)\n",
    "    \n",
    "    return\n",
    "\n",
    "def make_inv_operator(epoched_data, subj='default', \n",
    "                      subject_dir='default', \n",
    "                      coregistration='default', \n",
    "                      conductivity = (0.3, 0.006, 0.3), \n",
    "                      verbose = 'critical', SNR = 3, plot = False, \n",
    "                      domain = 'source'):\n",
    "    \"\"\"\n",
    "    Create the source spaces and the inverse operator\n",
    "    \n",
    "    Inputs: \n",
    "        epoched_data: The EEG data in the type MNE Epochs()\n",
    "        subj: subject name \n",
    "        subject_dir: The path to the subject directory \n",
    "        coregistration: the path to the -trans.fif file with MRI and EEG coregistation data\n",
    "        SNR: signal-to-noise level. 3 is default based on MNE Python\n",
    "        plot: whether to plot the source estimtes. \n",
    "        domain: 'source' or 'volume'\n",
    "        \n",
    "    Returns: \n",
    "        src: the source space data for the input domain\n",
    "        inv_operator: the inverse operator\n",
    "        lambda2: the regularization parameter based on SNR\n",
    "    \"\"\"\n",
    "    \n",
    "    # Silence all logs (semi-colons added on functions for good measure)\n",
    "    mne.utils.use_log_level(verbose)\n",
    "    mne.set_log_file(verbose)\n",
    "    \n",
    "    # If using default head models:\n",
    "    if subj == 'default':\n",
    "        path = str(mne.datasets.sample.data_path())\n",
    "        subj = 'sample'\n",
    "        subject_dir = path + r'/subjects'\n",
    "        coregistration = path + r'/MEG/sample/sample_audvis_raw-trans.fif'\n",
    "        \n",
    "    elif coregistration == 'default':\n",
    "        # Open GUI and get it?\n",
    "        # write it to and load it from the subject_dir\n",
    "        pass\n",
    "    \n",
    "    # Compute Noise Covariance Matrix\n",
    "    noise_cov = mne.compute_covariance(\n",
    "        epoched_data, tmax=0., method=['shrunk', 'empirical'], rank=None);\n",
    "    \n",
    "    # Make the BEM model\n",
    "    model = mne.make_bem_model(subject=subj, ico=4,\n",
    "                               conductivity=conductivity,\n",
    "                               subjects_dir=subject_dir);\n",
    "    bem = mne.make_bem_solution(model);\n",
    "    \n",
    "    if domain == 'source':\n",
    "        src = mne.setup_source_space(subject=subj, spacing='oct6', add_dist='patch',\n",
    "                             subjects_dir=subject_dir);\n",
    "    \n",
    "    elif domain == 'volume': \n",
    "        mri = os.path.join(subject_dir, subj, 'mri', 'T1.mgz')\n",
    "        src = mne.setup_volume_source_space(\n",
    "                    subject=subj, \n",
    "                    mri=mri,\n",
    "                    bem=bem,\n",
    "                    subjects_dir=subject_dir)\n",
    "    \n",
    "\n",
    "    # Compute the Forward Solution\n",
    "    fwd = mne.make_forward_solution(epoched_data.info, trans=coregistration, src=src, bem=bem,\n",
    "                                    meg=False, eeg=True, mindist=5.0, n_jobs=-1,);\n",
    "\n",
    "    # Get Inverse Opterator\n",
    "    evoked = epoched_data.average();\n",
    "    \n",
    "    inv_operator = mne.minimum_norm.make_inverse_operator(\n",
    "        info=evoked.info, \n",
    "        forward = fwd, \n",
    "        noise_cov = noise_cov);\n",
    "\n",
    "    # set signal-to-noise ratio (SNR) to compute regularization parameter (λ²)\n",
    "    lambda2 = 1. / SNR ** 2\n",
    "    \n",
    "    return src, inv_operator, lambda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7791ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subj12 Non-Modulated list index out of range\n",
      "Subj01 Decoupled-Sham list index out of range\n",
      "Subj04 Decoupled-Sham list index out of range\n",
      "Subj06 Decoupled-Sham list index out of range\n",
      "Subj07 Decoupled-Sham list index out of range\n",
      "Subj09 Decoupled-Sham list index out of range\n",
      "Subj15 Decoupled-Sham list index out of range\n",
      "Subj02 tFUS-GP list index out of range\n",
      "Subj01 tFUS-GP list index out of range\n",
      "Subj04 tFUS-GP list index out of range\n",
      "Subj07 tFUS-GP list index out of range\n",
      "Subj11 tFUS-GP list index out of range\n",
      "Subj13 tFUS-GP list index out of range\n",
      "Subj16 tFUS-GP list index out of range\n",
      "Subj19 tFUS-GP list index out of range\n"
     ]
    }
   ],
   "source": [
    "import os.path as op\n",
    "\n",
    "import mne\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from mne.minimum_norm import apply_inverse_epochs, apply_inverse\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "with open(path_to_file_paths) as f:\n",
    "    file_paths = json.load(f)  \n",
    "    \n",
    "key_mappings = {\"Decoupled-Sham\": \"Decoupled-Sham\", \"Non-Modulated\":\"Non-Modulated\", \n",
    "           \"tFUS-GC\":\"tFUS\", \"tFUS-GP\": \"US-Control\"}\n",
    "\n",
    "    \n",
    "domain = 'source'\n",
    "method = \"MNE\" # 'MNE', 'dSPM', 'sLORETA', and 'eLORETA'\n",
    "\n",
    "connectivities = {}\n",
    "granger_dfs = {}\n",
    "all_data = {}\n",
    "\n",
    "ESI_df = pd.DataFrame(columns = [\"subject\", \"condition\", \"data\", \"order\", \"scan_num\"])\n",
    "\n",
    "for condition in [\"tFUS-GC\", \"Non-Modulated\", \"Decoupled-Sham\", \"tFUS-GP\"]:\n",
    "    \n",
    "    connectivities[condition] = []\n",
    "    granger_dfs[condition] = []\n",
    "    all_data[condition] = []\n",
    "    \n",
    "    for subject in list(file_paths.keys()):\n",
    "        head_model = file_paths[subject][\"head_model\"]\n",
    "        subjects_dir = path_to_freesurfer + file_paths[subject][\"subject_dir\"]\n",
    "        session_num = file_paths[subject][\"RAS\"].split(r\"/\")[1]\n",
    "\n",
    "        trans = subjects_dir + fr'/{head_model}_{session_num}-trans.fif'\n",
    "        bem = subjects_dir + fr'/{head_model}_{session_num}-bem.fif'\n",
    "        mri = op.join(subjects_dir, head_model, 'mri', 'T1.mgz')\n",
    "        \n",
    "        # Set up the source space (either volume or source)\n",
    "        if domain == 'volume':\n",
    "            src = mne.setup_volume_source_space(\n",
    "                head_model, mri=mri, pos=10.0, bem=bem,\n",
    "                subjects_dir=subjects_dir,\n",
    "                add_interpolator=True,\n",
    "                verbose=True)\n",
    "        else:\n",
    "            src = mne.setup_source_space(subject=head_model, spacing='oct6', add_dist='patch',\n",
    "                                         subjects_dir=subjects_dir);\n",
    "\n",
    "        try: \n",
    "            epoch = mne.concatenate_epochs(preprocessed_df[\"data_epoch\"].loc[(preprocessed_df.subject == subject) \n",
    "                                                                             & (preprocessed_df.condition == condition)\n",
    "                                                                             & (preprocessed_df.hit == True)\n",
    "                                                                            ].to_list())\n",
    "            epoch.set_eeg_reference('average', projection=True)\n",
    "            \n",
    "            \n",
    "            # MAKE SURE THAT THE TMAX CORRESPONDS TO STIMULUS ONSET\n",
    "            noise_cov = mne.compute_covariance(epoch, tmax=0.0, method=['shrunk', 'empirical'], rank=None);\n",
    "\n",
    "            fwd = mne.make_forward_solution(epoch.info, trans=trans, src=src,\n",
    "                                            bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=-1)\n",
    "\n",
    "            # Inverse operator\n",
    "            info = epoch.info\n",
    "            inv = mne.minimum_norm.make_inverse_operator(info, fwd, noise_cov, loose=1, depth=0.8)\n",
    "\n",
    "            # Calculate source time series \n",
    "            snr = 3.0\n",
    "            lambda2 = 1.0 / snr ** 2\n",
    "                        \n",
    "            stcs = apply_inverse_epochs(epoch, inv, lambda2, method = 'MNE')\n",
    "            \n",
    "\n",
    "             # Get the labels of interest\n",
    "            labels = []\n",
    "            label_names = [\"V1\", \"V5\", \"SP\", \"IT\"]\n",
    "            for parc, FS_region in zip([\"BA_exvivo\", \"BA_exvivo\", \"aparc\", \"aparc\"], \n",
    "                                       [\"V1_exvivo-lh\", \"MT_exvivo-lh\", \"superiorparietal-lh\", \"inferiortemporal-lh\"]):\n",
    "                label = mne.read_labels_from_annot(head_model, parc=parc, \n",
    "                                                subjects_dir=subjects_dir, \n",
    "                                                regexp=FS_region)\n",
    "                \n",
    "                labels.append(label[0])\n",
    "            \n",
    "            # Extract Time series for labels of interest\n",
    "            for i, stc in enumerate(stcs):\n",
    "                ts = mne.extract_label_time_course(stcs = stc, \n",
    "                          labels = labels,\n",
    "                          src=src)\n",
    "                \n",
    "                ts = np.stack(ts)\n",
    "                ts = mne.filter.filter_data(ts, sfreq=100.0, l_freq=1, h_freq=40)\n",
    "                 \n",
    "                ESI_df.loc[len(ESI_df)] = {\"subject\": subject, \"condition\": condition, \n",
    "                           \"data\": ts, \"scan_num\": i + 1, \n",
    "                           \"order\": file_paths[subject]['order'][key_mappings[condition]]}\n",
    "        \n",
    "                \n",
    "                all_data[condition].append(ts)\n",
    "            \n",
    "        except IndexError as e:\n",
    "            print(subject, condition, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5da23",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c5cd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Outlier_Tests import IQR_Outlier_test\n",
    "\n",
    "data_dict_V5IT = {}\n",
    "data_dict_V5SP = {}\n",
    "data_dict_V5V1 = {}\n",
    "\n",
    "power = 'theta'\n",
    "\n",
    "if power == 'theta':\n",
    "    freq_bounds = [4, 8]\n",
    "elif power == 'alpha':\n",
    "    freq_bounds = [8, 12]\n",
    "\n",
    "# Z-Score Baseline (-20 to 0 ms)\n",
    "ESI_df['filtered_data'] = ESI_df['data'].apply(lambda x: (x - x[:,0:20].mean(axis=-1, keepdims=True))  / x[:,0:20].std(axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "# connectivity_df = pd.DataFrame(columns = ['V5SP', 'V5IT', 'V5V1', 'SPV1', 'SPIT', 'V1IT', 'Condition', 'Time', 'Subject'])\n",
    "\n",
    "\n",
    "connectivity_df = pd.DataFrame(columns = ['r', 'Connection', 'Condition', 'Subject', 'Time'])\n",
    "method = 'coh'\n",
    "mode = 'cwt_morlet'\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "   \n",
    "    for i, subject in enumerate(np.unique(ESI_df['subject'])):\n",
    "           \n",
    "            \n",
    "        for cond in np.unique(ESI_df['condition'].loc[ESI_df.subject == subject]):\n",
    "\n",
    "            all_data = np.stack(ESI_df['filtered_data'].loc[(ESI_df.subject == subject) & (ESI_df.condition == cond)].values)\n",
    "\n",
    "            V1_data = all_data[:, 0, :] # Stimulus onset to the end (500 ms)\n",
    "            V5_data = all_data[:, 1, :]\n",
    "            SP_data = all_data[:, 2, :]\n",
    "            IT_data = all_data[:, 3, :]\n",
    "\n",
    "            V1_data = mne.filter.filter_data(V1_data, sfreq=100.0, l_freq=freq_bounds[0], h_freq=freq_bounds[1])\n",
    "            V5_data = mne.filter.filter_data(V5_data, sfreq=100.0, l_freq=freq_bounds[0], h_freq=freq_bounds[1])\n",
    "            SP_data = mne.filter.filter_data(SP_data, sfreq=100.0, l_freq=freq_bounds[0], h_freq=freq_bounds[1])\n",
    "            IT_data = mne.filter.filter_data(IT_data, sfreq=100.0, l_freq=freq_bounds[0], h_freq=freq_bounds[1])\n",
    "\n",
    "\n",
    "            connection = \"V5SP\"\n",
    "            cons = []\n",
    "\n",
    "            for k in range(V5_data.shape[0]):\n",
    "                cons.append(scipy.stats.pearsonr(V5_data[k], SP_data[k])[0])\n",
    "\n",
    "            cons = np.abs(np.array(cons))\n",
    "\n",
    "            # Drop Outliers\n",
    "            cons = cons[~np.array(IQR_Outlier_test(cons))]\n",
    "#                 cons = cons[double_mads_from_median(cons) < 3.5]\n",
    "\n",
    "            for c in cons:\n",
    "\n",
    "                connectivity_df.loc[len(connectivity_df)] = {\"r\": c,   \n",
    "                                                             \"Connection\": connection,\n",
    "                                                             \"Condition\": cond, \"Subject\": subject, \n",
    "                                                             \"Time\": 0}\n",
    "\n",
    "\n",
    "            connection = \"V5IT\"\n",
    "            cons = []\n",
    "\n",
    "            for k in range(V5_data.shape[0]):\n",
    "                cons.append(scipy.stats.pearsonr(V5_data[k], IT_data[k])[0])\n",
    "\n",
    "            cons = np.abs(np.array(cons))\n",
    "\n",
    "            # Drop Outliers\n",
    "            cons = cons[~np.array(IQR_Outlier_test(cons))]\n",
    "#                 cons = cons[double_mads_from_median(cons) < 3.5]\n",
    "\n",
    "            for c in cons:\n",
    "\n",
    "                connectivity_df.loc[len(connectivity_df)] = {\"r\": c,   \n",
    "                                                             \"Connection\": connection,\n",
    "                                                             \"Condition\": cond, \"Subject\": subject, \n",
    "                                                             \"Time\": 0}\n",
    "\n",
    "            connection = \"V5V1\"\n",
    "            cons = []\n",
    "\n",
    "            for k in range(V5_data.shape[0]):\n",
    "                cons.append(scipy.stats.pearsonr(V5_data[k], V1_data[k])[0])\n",
    "\n",
    "            cons = np.abs(np.array(cons))\n",
    "\n",
    "            # Drop Outliers\n",
    "            cons = cons[~np.array(IQR_Outlier_test(cons))]\n",
    "#                 cons = cons[double_mads_from_median(cons) < 3.5]\n",
    "\n",
    "            for c in cons:\n",
    "\n",
    "                connectivity_df.loc[len(connectivity_df)] = {\"r\": c,   \n",
    "                                                             \"Connection\": connection,\n",
    "                                                             \"Condition\": cond, \"Subject\": subject, \n",
    "                                                             \"Time\": 0}\n",
    "\n",
    "            connection = \"SPV1\"\n",
    "            cons = []\n",
    "\n",
    "            for k in range(V5_data.shape[0]):\n",
    "                cons.append(scipy.stats.pearsonr(V1_data[k], SP_data[k])[0])\n",
    "\n",
    "            cons = np.abs(np.array(cons))\n",
    "\n",
    "            # Drop Outliers\n",
    "            cons = cons[~np.array(IQR_Outlier_test(cons))]\n",
    "#                 cons = cons[double_mads_from_median(cons) < 3.5]\n",
    "\n",
    "            for c in cons:\n",
    "\n",
    "                connectivity_df.loc[len(connectivity_df)] = {\"r\": c,   \n",
    "                                                             \"Connection\": connection,\n",
    "                                                             \"Condition\": cond, \"Subject\": subject, \n",
    "                                                             \"Time\": 0}\n",
    "            connection = \"V1IT\"\n",
    "            cons = []\n",
    "\n",
    "            for k in range(V5_data.shape[0]):\n",
    "                cons.append(scipy.stats.pearsonr(V1_data[k], IT_data[k])[0])\n",
    "\n",
    "            cons = np.abs(np.array(cons))\n",
    "\n",
    "            # Drop Outliers\n",
    "            cons = cons[~np.array(IQR_Outlier_test(cons))]\n",
    "#                 cons = cons[double_mads_from_median(cons) < 3.5]\n",
    "\n",
    "            for c in cons:\n",
    "\n",
    "                connectivity_df.loc[len(connectivity_df)] = {\"r\": c,   \n",
    "                                                             \"Connection\": connection,\n",
    "                                                             \"Condition\": cond, \"Subject\": subject, \n",
    "                                                             \"Time\": 0}\n",
    "            connection = \"SPIT\"\n",
    "            cons = []\n",
    "\n",
    "            for k in range(V5_data.shape[0]):\n",
    "                cons.append(scipy.stats.pearsonr(IT_data[k], SP_data[k])[0])\n",
    "\n",
    "            cons = np.abs(np.array(cons))\n",
    "\n",
    "            # Drop Outliers\n",
    "            cons = cons[~np.array(IQR_Outlier_test(cons))]\n",
    "#                 cons = cons[double_mads_from_median(cons) < 3.5]\n",
    "\n",
    "            for c in cons:\n",
    "\n",
    "                connectivity_df.loc[len(connectivity_df)] = {\"r\": c,   \n",
    "                                                             \"Connection\": connection,\n",
    "                                                             \"Condition\": cond, \"Subject\": subject, \n",
    "                                                             \"Time\": 0}\n",
    "                \n",
    "for connection in np.unique(connectivity_df.Connection):\n",
    "    connectivity_df.loc[connectivity_df.Connection == connection].to_csv(f\"/Users/jkosnoff/Downloads/Rerun_{connection}_cor_{power}-IQR.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
